---
title: "DADA2 - Boyd 16S"
output: html_notebook
---

```{r}
# if (!requireNamespace("BiocManager", quietly = TRUE))
#      install.packages("BiocManager")
#   BiocManager::install("phyloseq")

```


```{r}
ncores = 16
```

```{r}
library(dada2)
library(tidyr)
library(dplyr)
library(phyloseq)
```


```{r}
seqDIR = '/project/mmicrobe/Boyd/16S/20211210/lane1'
path = seqDIR
list.files(path)

#location of taxonomy database
TrainingSet = '~/databases/silva_nr99_v138.1_train_set.fa.gz'
SpeciesTraining = '~/databases/silva_species_assignment_v138.1.fa.gz'

# Sample data table
SamTab = '/project/mmicrobe/Boyd/16S/BoydSampleData.csv'

# output folder
OutFolder = '/project/mmicrobe/Boyd/16S/DADA2files'
```

# Filter and trim

```{r}
#Collect forward and reverse names in order
fastqFs <- sort(list.files(path, pattern="._L001_R1_001.fastq"))
fastqRs <- sort(list.files(path, pattern="._L001_R2_001.fastq"))
if(length(fastqFs) != length(fastqRs)) stop("Forward and reverse files do not match.")
```

# Plot quality profiles and forward and reverse reads
## Forward reads
```{r}
plotQualityProfile(file.path(path,fastqFs[1:4]))
```
## Reverse reads
```{r}
plotQualityProfile(file.path(path,fastqRs[1:4]))
```
* Forward reads look good, may not need to truncate
* Reverse reads also look very good, truncate to 225

# Perform filtering and trimming

```{r}
filtpathF <- file.path(path, "filtered_F") # Filtered forward files go into the pathF/filtered/ subdirectory
filtpathR <- file.path(path, "filtered_R") # ...
```

Do primers need to be filtered? 
Primers are: 16S	
515F: GTGYCAGCMGCCGCGGTAA		 
806R: GGACTACNVGGGTWTCTAAT	 	 
```{r}

readLines(file.path(path,fastqFs[1]), n = 4)
readLines(file.path(path,fastqRs[1]), n = 4)
#path
#fastqFs[1]
```
Primers appear to be absent


```{r}
# Filtering: THESE PARAMETERS ARENT OPTIMAL FOR ALL DATASETS
out = filterAndTrim(fwd=file.path(path, fastqFs), filt=file.path(filtpathF, fastqFs),
              rev=file.path(path, fastqRs), filt.rev=file.path(filtpathR, fastqRs),
              truncLen=c(225,200), maxEE=2, truncQ=2, maxN=0, rm.phix=TRUE, 
              compress=TRUE, verbose=TRUE, multithread=ncores)
```

```{r}
head(out)
saveRDS(out, file.path(OutFolder,'out.rds'))
```

```{r}
filtFs <- list.files(filtpathF, pattern=".fastq.gz", full.names = TRUE)
filtRs <- list.files(filtpathR, pattern=".fastq.gz", full.names = TRUE)
sample.names <- sapply(strsplit(basename(filtFs), "_L001_R1_001.fastq.gz"), `[`, 1) # Assumes filename = samplename_XXX.fastq.gz
sample.namesR <- sapply(strsplit(basename(filtRs), "_L001_R2_001.fastq.gz"), `[`, 1) # Assumes filename = samplename_XXX.fastq.gz
if(!identical(sample.names, sample.namesR)) stop("Forward and reverse files do not match.")
names(filtFs) <- sample.names
names(filtRs) <- sample.names
sample.names
```

# Learn errors

```{r}
set.seed(100)
# Learn forward error rates
errF <- learnErrors(filtFs, multithread=ncores)
# Learn reverse error rates
errR <- learnErrors(filtRs, multithread=ncores)
```

```{r}
plotErrors(errF)
errF_File = file.path(OutFolder,"errF.rds")
saveRDS(errF, errF_File)

errR_File = file.path(OutFolder,"errR.rds")
saveRDS(errR, errR_File)
```

# Sequence Inference


## Big data version
```{r}
# Sample inference and merger of paired-end reads
mergers <- vector("list", length(sample.names))
names(mergers) <- sample.names
for(sam in sample.names) {
  cat("Processing:", sam, "\n")
    derepF <- derepFastq(filtFs[[sam]])
    ddF <- dada(derepF, err=errF, multithread=ncores)
    derepR <- derepFastq(filtRs[[sam]])
    ddR <- dada(derepR, err=errR, multithread=ncores)
    merger <- mergePairs(ddF, derepF, ddR, derepR)
    mergers[[sam]] <- merger
}
rm(derepF); rm(derepR)

mergers_File = file.path(OutFolder,"mergers.rds")
saveRDS(mergers, mergers_File)
```


# Checkpoint - read mergers file, construct sequence table and remove chimeras

```{r}
mergers_File = file.path(OutFolder,"mergers.rds")
mergers = readRDS(mergers_File)
seqtab <- makeSequenceTable(mergers)

outFile = file.path(OutFolder,"seqtab.rds")
saveRDS(seqtab, outFile)
```


```{r}
dim(seqtab)
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
```
## Subset to proper length

```{r}
seqtab2 <- seqtab[,nchar(colnames(seqtab)) %in% seq(252,254)]

table(nchar(getSequences(seqtab2)))
```
## Remove chimeras

```{r}

seqtab.nochim <- removeBimeraDenovo(seqtab2, method="consensus", multithread=TRUE, verbose=TRUE)

```

```{r}
dim(seqtab.nochim)
```

```{r}
outFile = file.path(OutFolder,"seqtab-nochim.rds")

saveRDS(seqtab.nochim, outFile)
```

# Checkpoint read seqtab.nochim
```{r}
CheckPoint = file.path(OutFolder,"seqtab-nochim.rds")

seqtab.nochim = readRDS(CheckPoint)
```

```{r}
paste("Fraction of reads not attributed to chimeras:", sum(seqtab.nochim)/sum(seqtab))
```

# Track reads through pipeline
```{r}
getN <- function(x) sum(getUniques(x))
#track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
track <- cbind(out, sapply(mergers, getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered",  "merged", "nonchim")
rownames(track) <- sample.names
head(track)

```


```{r}
track %>% as.data.frame() %>% mutate(sample = row.names(.)) %>% dplyr::arrange(desc(nonchim))
```
```{r}
outFile = file.path(OutFolder,'Tracking.txt')
write.table(track, file = outFile, sep = "\t")
```

## double check

```{r}
st2 = collapseNoMismatch(seqtab.nochim)
```

```{r}
dim(seqtab.nochim)
dim(st2)
```
```{r}
outFile = file.path(OutFolder,"st2.rds")

saveRDS(st2, outFile)
```

### Clean up memory
```{r}
rm(ddF, ddR, errF, errR, merger, mergers, out, seqtab, seqtab.nochim, seqtab2, track)
rm(st2)
```

# Checkpoint

```{r}
CheckPoint = file.path(OutFolder,"st2.rds")

seqtab.nochim = readRDS(CheckPoint)
```

# Assign Taxonomy
```{r}
 taxa <- assignTaxonomy(seqtab.nochim, TrainingSet, multithread=ncores)
```


```{r}
outFile = file.path(OutFolder,'taxa.rds')
outFile
saveRDS(taxa, outFile)
```

# Checkpoint read taxa file

```{r}
CheckPoint = file.path(OutFolder,"taxa.rds")
taxa = readRDS(CheckPoint)

```

```{r}
#inspect taxonomic assignment

taxa.print <- taxa # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)
```
```{r}
dim(taxa)
```
```{r}
names(seqtab.nochim)[1:4]
```

# Hand off to phyloseq

```{r}
# read sample data
samdf = read.delim(file = SamTab, header = TRUE, sep = ',')
head(samdf)
row.names(samdf) = samdf$X
samdf = samdf[,-1]
rownames(samdf)[1:4]
```
```{r}
dim(samdf)
```

```{r}
#rename taxa
taxa = cbind(taxa, row.names(taxa))

colnames(taxa)[7] = "Seq"
```

```{r}
rownames(seqtab.nochim)[1:50]
```
```{r}
rownames(seqtab.nochim) %>% length()
```

```{r}
rownames(samdf) %>% length
```
```{r}
toremove = setdiff(rownames(seqtab.nochim),rownames(samdf))
toremove
```
```{r}
allsamples = rownames(samdf)
allsamples = allsamples[!(allsamples %in% toremove)]
seqtab.nochim2 = seqtab.nochim[allsamples,]
seqtab.nochim2 %>% dim()
```
# Create phyloseq object
```{r}
ps <- phyloseq(otu_table(seqtab.nochim2, taxa_are_rows=FALSE), 
               sample_data(samdf), 
               tax_table(taxa))
ps
```
```{r}
CheckOTU = taxa_names(ps)[150]
CheckOTU
```
```{r}
ps %>% prune_taxa(CheckOTU,.) %>% otu_table()
```
```{r}
sample_names(ps)[1:55]
```

## Rename taxa
```{r}
new.names <- paste0("ASV", seq(ntaxa(ps))) # Define new names ASV1, ASV2,
seqs <- taxa_names(ps) # Store sequences
names(seqs) <- new.names # Make map from ASV1 to full sequence
taxa_names(ps) <- new.names # Rename to human-friendly format
taxa_names(ps)[1:10]
```

```{r}
subset_taxa(ps, Seq == CheckOTU, TRUE) %>% 
    prune_samples("2-F04-E18R_S132",.) %>%
    otu_table()
```
* Sample sum matches

```{r}
taxa_sums(ps)[1:10]
```
## Save full object
```{r}
ps
saveRDS(ps, file = file.path(OutFolder, 'BoydPhyloseqFull.rds')) 
```

## Threshold to remove minor sequences: sum(x > 3) > 3

```{r}
tax_table(ps) %>% head
ps.thresh = filter_taxa(ps, function(x) sum(x > 3) > 3, TRUE)
ps.thresh
```

```{r}
rm(ps)
```

### Remove and save sequences from tax_table
*removing sequences will greatly speed up psmelt and subsequent operations
```{r}
Seqs_df = cbind(rownames(tax_table(ps.thresh)), tax_table(ps.thresh)[,'Seq'])
```

```{r}
colnames(Seqs_df)[1:2] = c("ASV", "Seq")
head(Seqs_df)
```

```{r}
taxa_df = tax_table(ps.thresh)[,1:6]
head(taxa_df)
```
```{r}
#save table of seqs
write.table(Seqs_df, file = file.path(OutFolder,'taxa_seqs.txt'), sep = '\t')
```

```{r}
# save fasta file of seqs
outfile = file.path(OutFolder,'seqs_thresh.fasta')

SeqNames = Seqs_df[,'ASV'] %>%
    as.list()
SeqNames[1:4]
seqs = Seqs_df[,'Seq'] %>% as.list()
seqs[1:4]
seqinr::write.fasta(sequences = as.list(seqs), names = SeqNames, file.out = outfile)
```

### Save thresholded phyloseq with simplified taxa table
```{r}
head(taxa_df)
```
```{r}
tax_table(ps.thresh) %>% head
```
```{r}
tax_table(ps.thresh) = taxa_df
head(tax_table(ps.thresh))
```

```{r}
ps.thresh
```

```{r}
outfile = file.path(OutFolder,'Boyd_phyloseq_thresh.rds')
saveRDS(ps.thresh, file = outfile)
```

# Next steps: sequences will be used to build a tree in a python script and then tree will be re-united with phyloseq object in subsequent notebook

```{r}
sessionInfo()
```
